{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c8f66a0-fde4-4efb-ada5-b618728bd9ab",
   "metadata": {},
   "source": [
    "## Approach #1: Simplified Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfc6ad5f-95b4-4cfa-b605-f343b94a3a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f48ff9a8-9cfd-4ac9-82bb-691a67c1bb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# pick one as your query\n",
    "query = inputs[1]\n",
    "\n",
    "# initialize attention scores\n",
    "attention_scores_2 = torch.empty(inputs.shape[0])\n",
    "\n",
    "# compute its value in a simplified manner.. just take\n",
    "# dot products\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attention_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attention_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5108077b-fef5-4308-be4d-37aa4d84aae7",
   "metadata": {},
   "source": [
    "Next step: convert from 'scores' -> 'weights' (normalized)\n",
    "\n",
    "Attention **Scores** are respresented by $\\omega$\n",
    "\n",
    "Attention **Weights** are represented by $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ee7e1f-0ffa-4f58-a8e0-7b52d7eb1906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attention_scores_2 / attention_scores_2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cce57da-4151-43a3-86c7-ef1dbc611fc2",
   "metadata": {},
   "source": [
    "It's preferable to normalize using Softmax instead, to manage extreme values and provide favorable gradients during training. \n",
    "\n",
    "Let's try that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a993a82-222a-48b4-946b-595165a4a20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Summed: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)    \n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attention_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Summed:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "727403c9-c7d0-4d99-8b12-3cd56320ddd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Summed: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attention_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Summed:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56414c9b-c2cc-412f-98ac-ef3f80a79ef0",
   "metadata": {},
   "source": [
    "Final step! We have the **attention weights**, and now need to combine them into a **context vector**. \n",
    "\n",
    "In this simplified version, we'll just sum them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dba49d4d-304d-46d9-8833-0af61cc09b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e4bec1-0b93-43ed-ad50-e0c8608dbd93",
   "metadata": {},
   "source": [
    "Same thing, but now we'll do it for ALL of the input tokens, treating each input as a **query**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f806e20-7e6d-4ba2-9878-65d20333fd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "d = len(inputs)\n",
    "attn_scores = torch.empty(d, d)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fae41c51-4c76-43c0-abc4-89dbeece5051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T # matrix multiplication ftw!\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a5dbf72-fed8-42b8-a15b-ad670ae6a797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d164e5a-f283-4ae5-ae5b-f4ed74410d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.1385, 0.1390, 0.1435, 0.1526, 0.1385],\n",
       "        [0.2006, 0.2379, 0.2369, 0.2074, 0.1958, 0.2184],\n",
       "        [0.1981, 0.2333, 0.2326, 0.2046, 0.1975, 0.2128],\n",
       "        [0.1242, 0.1240, 0.1242, 0.1462, 0.1367, 0.1420],\n",
       "        [0.1220, 0.1082, 0.1108, 0.1263, 0.1879, 0.0988],\n",
       "        [0.1452, 0.1581, 0.1565, 0.1720, 0.1295, 0.1896]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(attn_scores, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c51201a-ee70-4ead-99ce-7d57b737ce41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(attn_scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55170355-8be2-4af9-a731-0ed130c41108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.softmax(attn_scores, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "646a2c2e-c05e-48c2-bcb3-2ab816f8bc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.5617)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(attn_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "107e8a0b-bccc-4b66-a61f-a72963a8b74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(attn_weights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f8bceae-3cf6-4eff-8a32-54bc357f18e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a3bc0a9-72cf-45a1-a8ec-ae8385ebdddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9220, 1.2970, 1.2788, 0.7974, 0.7540, 0.9508])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights.sum(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e202fc3-fbb9-42ee-885d-663ad28380fd",
   "metadata": {},
   "source": [
    "I'll have to keep practicing to get a good intuition on dim. Dim=0 seems like it refers to the \"first\" dimension, whereas dim=1 is the \"second\" dimension. dim=-1 is the \"last\" dimension, is the same as the second in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a593445-653c-4695-abcc-105c223ce1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_context_vecs = attn_weights @ inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb34d9da-1350-4195-9316-c0edd89b2e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd715bf3-63e9-4334-bf4c-3da0402a49e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.6141, 1.6617, 1.6598, 1.6112, 1.5847, 1.6326])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_context_vecs.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "460eacf2-e3b8-40d7-8adb-a00569fd7a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4419, 0.6515, 0.5683])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d6f80f6-ff93-4f8d-bcfc-5b9cb5bdc4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True, False])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2.eq(all_context_vecs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05de5698-597a-47e5-a3d5-e990d0cfc4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(context_vec_2, all_context_vecs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d7e365-a3a0-407f-88bd-aee2d8df2c1a",
   "metadata": {},
   "source": [
    "## Approach #2: Self-attention with trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adab7dc-e603-4ac8-bbcf-d28e2715abee",
   "metadata": {},
   "source": [
    "### aka \"scaled-dot product attention\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a614221-f3a8-472b-be9c-20e04c4cd2e5",
   "metadata": {},
   "source": [
    "We will add three weight matrices which are trainable: $W_q$, $W_k$, $W_v$.\n",
    "\n",
    "Multiplying the input $x^{(i)}$ by a weight projects it into that space. For example:\n",
    "\n",
    "- $x^{(i)} * W_k = k^{(i)}$ ... project into the \"key\" space\n",
    "- $x^{(i)} * W_v = v^{(i)}$ ... project into the \"value\" space\n",
    "- $x^{(i)} * W_q = q^{(i)}$ ... project into the \"query\" space\n",
    "\n",
    "Recall that $x$ is an **embedding** of the text token, with a certain dimension.\n",
    "When we project into the \"output\" space, it is an embedding of some dimension which needn't be the same as $x$'s dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47d1fc32-0c0c-4614-baf3-a9965f8248ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] # second input element\n",
    "d_in = inputs.shape[1] # input embedding size\n",
    "d_out = 2 # output embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fc0aa23-ac38-48aa-9431-bf2ae4466d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) # nit: we are ignoring grad for now for simplicity. we'll want it later!\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "086cbb41-b236-4b01-abe0-fc8484327de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n"
     ]
    }
   ],
   "source": [
    "print(W_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48a410ab-60b3-4314-b7ca-733d975722a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38eb0e12-feab-416d-afb2-c38277b6addf",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = inputs @ W_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a455bb5a-233b-469b-81c7-cd47d1e706c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = inputs @ W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b41d32f6-3fc2-40a5-994b-375fe1163c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"keys.shape:\", keys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d4574d2-c952-4bc3-aa34-c0d1fe7a7a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2348e9c4-f588-40a7-b387-b19d63841e5e",
   "metadata": {},
   "source": [
    "To compute an attention score for a given token idx (\"queried token\"), we multiply the query by the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc6bc445-886f-464e-87b6-97dec845d5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "attn_score_22 = query_2.dot(keys[1]) # naming is 1-indexed in math variable name land and 0-indexed in computer code land\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946db767-4e87-4bec-8eb3-b7dd0d5d02c4",
   "metadata": {},
   "source": [
    "Now let's get the same value via matrix multiplication..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0b2c9a1-11bf-44b2-bf98-be35d583ecde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c7f6510-51d9-4866-a049-f25df83ac3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(attn_score_22, attn_scores_2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09560c49-17c3-460f-8227-bde305086b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "615fe530-4f95-487a-9ebc-799beb30429b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1401, 0.2507, 0.2406, 0.1157, 0.0687, 0.1842])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45640b56-f227-45e6-ba14-de9133d9721b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "# that was wrong actually! We do a adjustment to scale it further\n",
    "# (1) NEW: divide by the sq root of the embedding dimension of the keys\n",
    "# (2) take the softmax\n",
    "\n",
    "# The reason for the normalization by the embedding dimension size \n",
    "# is to improve the training performance by avoiding small gradients.\n",
    "\n",
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e69b71-8ab5-4684-94d6-28162e891b50",
   "metadata": {},
   "source": [
    "Awesome, now we have the **Attention weights** $\\alpha_{2i}$, and we want to compute the **Context Vector** $Z^{(2)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85f23b76-4995-4d42-b49a-aa44f86b6508",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vec_2 = attn_weights_2 @ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b3ba267-1db9-4d37-b998-43590a491d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1855, 0.8812],\n",
       "        [0.3951, 1.0037],\n",
       "        [0.3879, 0.9831],\n",
       "        [0.2393, 0.5493],\n",
       "        [0.1492, 0.3346],\n",
       "        [0.3221, 0.7863]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8e775cc-a650-42d8-82d9-846ce3b1ddb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3061, 0.8210])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "adae3758-0173-43fe-901d-4189f5a7bb4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed660032-aeb5-4766-9148-d66db8f15158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41a06d32-3bcf-4e59-b899-52da3466af33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e226f87c-73c2-4435-9ec0-34b623fc8a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1855, 0.3951, 0.3879, 0.2393, 0.1492, 0.3221],\n",
       "        [0.8812, 1.0037, 0.9831, 0.5493, 0.3346, 0.7863]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "027d05a6-192c-4918-a006-cb189ce1769b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3061)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2.dot(values.T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "818ca929-798f-4dcb-bdf5-9abb07eaef75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8210)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2.dot(values.T[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2986e0df-db5d-46e1-b8ac-225f6263d793",
   "metadata": {},
   "source": [
    "Let's now implement class to compute self-attention for all queried tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c9d5ff0-6732-4b94-a32f-5aad5608e2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" given inputs `x` (embedding dim = d_in),\n",
    "            execute the whole 'attention' module, \n",
    "            returns the computed context vectors (dim = d_out)\"\"\"\n",
    "        queries = x @ self.W_query\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        # scores (\"omega\") are queries * keys\n",
    "        attn_scores = queries @ keys.T\n",
    "        \n",
    "        # weights (\"alpha\") are normalized via softmax AND sqroot of embedding size\n",
    "        d_k = keys.shape[-1]\n",
    "        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ebcdcb3d-fc8d-4581-b8eb-ec246ef59f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b9c17c2-b1b0-475d-a29c-95c3111aec15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3061, 0.8210])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd7b0f66-e5df-474f-b290-b5ac2240347d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(context_vec_2, sa_v1(inputs)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dcb2a7-1e9b-4838-bef1-ced54eb373b3",
   "metadata": {},
   "source": [
    "Let's tweak our implementation to use `nn.Linear`, which works well because\n",
    "- without a bias unit, it just performs matrix multiplication\n",
    "- it has a optimizated random weights initialization scheme vs `torch.rand` -> stable and effective model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15ec5289-0905-4324-97d1-4cbbe27c0165",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        d_k = keys.shape[-1]\n",
    "        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f3622026-c707-4b92-b4be-c859c6577835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12992a-a150-4ca9-b229-7dcc93e38f29",
   "metadata": {},
   "source": [
    "Ex 3.1 -- try to make v1 and v2 output the same thing by transferring the weights from V2 into a V1 instance...\n",
    "\n",
    ">  we can transfer the weight matrices from a SelfAttention_v2 object to a SelfAttention_v1, such that both objects then produce the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6ee853c-ca14-408a-8e5a-7ae4ac08cbee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4058, -0.4704,  0.2368],\n",
       "        [ 0.2134, -0.2601, -0.5105]], requires_grad=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v2.W_key.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57163bff-840d-4a13-9eef-28ffc3d9e13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.1366, 0.1025],\n",
       "        [0.1841, 0.7264],\n",
       "        [0.3153, 0.6871]], requires_grad=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v1.W_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf8fe81d-121d-4ce1-a864-21e555968db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(sa_v2.W_key.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c901f984-ee1c-4532-ac5f-9fddfd95f36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4058, -0.4704,  0.2368],\n",
      "        [ 0.2134, -0.2601, -0.5105]])\n"
     ]
    }
   ],
   "source": [
    "print(sa_v2.W_key.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b3424125-6527-485e-8cc8-04379ce5d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_v1.W_key.data = sa_v2.W_key.weight.data.T\n",
    "sa_v1.W_value.data = sa_v2.W_value.weight.data.T\n",
    "sa_v1.W_query.data = sa_v2.W_query.weight.data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4de53e5d-955d-42f4-bcf3-3e6254eb427c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0739,  0.0713],\n",
       "        [-0.0748,  0.0703],\n",
       "        [-0.0749,  0.0702],\n",
       "        [-0.0760,  0.0685],\n",
       "        [-0.0763,  0.0679],\n",
       "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v1(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff057161-c0aa-4cbf-bec0-3844c0535e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True],\n",
       "        [True, True],\n",
       "        [True, True],\n",
       "        [True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(sa_v1(inputs).data, sa_v2(inputs).data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30c1dc1-b5b2-4c48-a50a-4fcbf8db7d94",
   "metadata": {},
   "source": [
    "The main trick to transfering were:\n",
    "- accessing the weights via `.weight.data`... `.weight` alone is a `Parameter` and has other information about the `grad_fn`.\n",
    "- transposing the weight's data (`.weight.data.T`) to pass it from V2 to V1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1586d5-f4b6-4d0d-8aef-275c5b9efcfa",
   "metadata": {},
   "source": [
    "## Approach #3: Causal attention (hiding future words)\n",
    "\n",
    "bonus: We'll also use NN dropout here to reduce overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af5e21c-f61b-402f-9faa-5c815c1dc569",
   "metadata": {},
   "source": [
    "The general way we'll do \"casual attention\" is by applying a **mask**.\n",
    "\n",
    "1. (simple) mask along the diagonal with 0s\n",
    "  ... then compute attention weights as before\n",
    "2. (fancier) mask along the diagonal with $-\\infty$\n",
    "  ... this allows lets us skip one matrix multiplication, since we're applying softmax anyway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39dcfac-f00b-47d5-9417-1bf2fea1be1e",
   "metadata": {},
   "source": [
    "Let's start with the simple mask..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "111b1cb0-8589-4fe4-8969-f0442723df17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)     #1\n",
    "keys = sa_v2.W_key(inputs) \n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1d04dbc0-58e6-44a6-8305-1175a5a25aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2899,  0.0716,  0.0760, -0.0138,  0.1344, -0.0511],\n",
       "        [ 0.4656,  0.1723,  0.1751,  0.0259,  0.1771,  0.0085],\n",
       "        [ 0.4594,  0.1703,  0.1731,  0.0259,  0.1745,  0.0090],\n",
       "        [ 0.2642,  0.1024,  0.1036,  0.0186,  0.0973,  0.0122],\n",
       "        [ 0.2183,  0.0874,  0.0882,  0.0177,  0.0786,  0.0144],\n",
       "        [ 0.3408,  0.1270,  0.1290,  0.0198,  0.1290,  0.0078]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d1395eee-a65d-491c-9c7c-7ee3e5417658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "43394910-bccc-4096-91c9-173b7936beb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "# tril returns the \"lower triangle\" of the matrix, including the diagonal \n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0d381fb3-b645-4fe6-8d34-8a76124dabe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cea10b-72b6-4577-80c4-8906db30179f",
   "metadata": {},
   "source": [
    "We now want to renormalize each row in the masked attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bc302916-7591-49f0-bf94-f311238f0066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921],\n",
      "        [0.3700],\n",
      "        [0.5357],\n",
      "        [0.6775],\n",
      "        [0.8415],\n",
      "        [1.0000]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "print(row_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e5a91275-bf5d-4f0b-a07d-f5be38ebb9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a711b1-895e-42c8-9bdd-eacf2b260de9",
   "metadata": {},
   "source": [
    "Let's try the fancier mask with the matrix math optimization..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "41552794-ac43-4725-b89f-65b095e4aaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "84d9bf95-54f4-44e4-addb-7f69afd15657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "47ca2093-97c9-4b29-9e68-19ed93d2c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vec = attn_weights @ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3d227d3a-9c5f-4628-b9bc-72fddc874e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1855, 0.8812],\n",
       "        [0.2795, 0.9361],\n",
       "        [0.3133, 0.9508],\n",
       "        [0.2994, 0.8595],\n",
       "        [0.2702, 0.7554],\n",
       "        [0.2772, 0.7618]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8794516-5019-45a2-928d-5cc9f83e7cd2",
   "metadata": {},
   "source": [
    "Let's add **dropout** to randomly ignore some hidden layer units during training. This helps prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1d80baa6-2b46-478f-821e-be9c4917f820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6,6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f972e1e9-7be3-40c6-b132-f9c7c7fc6662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af76a81-2f7a-4413-9eb5-2945de093c46",
   "metadata": {},
   "source": [
    "We didn't add dropout yet, but we've built the intuition of how it will work.\n",
    "\n",
    "Let's do one more thing. We want to ensure the `SelfAttention` Python class we create is able to handle **batched inputs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "030d85e3-2d41-4484-b9e5-db0994d64987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a76b2c55-b539-4f30-b3d0-0651e05a1c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n"
     ]
    }
   ],
   "source": [
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e8f3530d-5d02-4109-88e4-123d2690fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # ensures our buffers are moved to CPU, GPU as needed \n",
    "        self.register_buffer('mask', \n",
    "                             torch.triu(torch.ones(context_length, context_length), \n",
    "                                        diagonal=1)\n",
    "                            )\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # previously, we just did keys.T\n",
    "        # to handle a batch we need to transponse the later 2 dims but keep the first\n",
    "        attn_scores = queries @ keys.transpose(1,2) \n",
    "        \n",
    "        # This is subtly change from our experiment above.\n",
    "        # [:num_tokens, :num_tokens] .. ?? only update these ??\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        # masked_fill_  -> update in place\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights  = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e66df075-271a-40a1-a192-037e404ba6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.shape: torch.Size([2, 6, 3])\n",
      "context_vecs.shape torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(\"batch.shape:\", batch.shape)\n",
    "context_length = batch.shape[1] \n",
    "    # [0] - number of inputs (\"batch size\")\n",
    "    # [1] - number of tokens per input\n",
    "    # [2] - size of the input embedding\n",
    "dropout_ratio = 0.0\n",
    "ca = CausalAttention(d_in, d_out, context_length, dropout_ratio)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape\", context_vecs.shape)\n",
    "    # [0] - number of inputs (\"batch size\")\n",
    "    # [1] - number of tokens per input\n",
    "    # [2] - size of the output embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d55568-881b-4cdb-a301-7aee9282128d",
   "metadata": {},
   "source": [
    "## Approach #4: Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc60d411-44a2-4a09-91c4-2a284739ce84",
   "metadata": {},
   "source": [
    "Intuitively, our multi-head attention model can be thought of as several CausalAttention modules stacked. We execute then all and concat their results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2b240805-bbda-421a-a889-f4a3f32f69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for n in range(num_heads)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h.forward(x) for h in self.heads], dim = -1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7d3133ae-a919-4a41-bdd7-226f4fba53de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # number of tokens\n",
    "d_in = batch.shape[2] # input embedding size\n",
    "d_out = 2\n",
    "droput_ratio = 0.0\n",
    "num_heads = 2\n",
    "\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, droput_ratio, num_heads)\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37285e06-0054-4f60-a745-2ca1aad1d098",
   "metadata": {},
   "source": [
    "The final dim of the context vecs is now 4. Because: `4 = (2 heads) * (output embedding dim of 2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fcc235-5110-4387-8228-071792a5e810",
   "metadata": {},
   "source": [
    "The remaining work is to take this idea and make it efficient by combining the iterative `forward()` steps into a single one, with one matrix mult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa95d23-c8dc-4ca4-9639-62ddd88ab87b",
   "metadata": {},
   "source": [
    "## Approach #4 (variant): Efficient multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "761a255a-777a-4c5f-9e93-584d90173567",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout: float, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            f\"d_out ({d_out}) must be divisible by num_heads ({num_heads})\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "        self.out_proj = nn.Linear(d_out, d_out) # Linear layer to combine head outputs\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # b is number inputs in a batch\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # after multiplying by weights, the shape of these is: \n",
    "        # (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x) \n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # view() aka \"reshape\"\n",
    "        # We can think of this as breaking up the single matrix into multiple, one per head (num_heads)\n",
    "        # recall that `d_out = num_heads * head_dim`\n",
    "        # after running view(), the new dimensions are: (b, num_tokens, num_heads, head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        \n",
    "        # transpose results in shape \n",
    "        # before: (b, num_tokens, num_heads, head_dim)\n",
    "        #                 -> \n",
    "        # after:  (b, num_heads, num_tokens, head_dim)\n",
    "        queries = queries.transpose(1,2) \n",
    "        keys = keys.transpose(1,2) \n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        # 1st, we called .T \n",
    "        # 2nd, we called .transpose(1,2) to handle batches (idx=0)\n",
    "        # now, we call   .transpose(2,3) to handle batches (idx=0) with multiple heads (idx=1)\n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "        \n",
    "        # apply mask for causal attention\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        # normalize\n",
    "        d_k = keys.shape[-1]\n",
    "        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n",
    "\n",
    "        # dropout\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # compute context vector\n",
    "        # \n",
    "        # we transpose to convert from:\n",
    "        # before: (b, num_heads, num_tokens, head_dim)\n",
    "        #     ->\n",
    "        # after: (b, num_tokens, n_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1,2) \n",
    "\n",
    "        # combine the heads\n",
    "        # before: (b, num_heads, num_tokens, head_dim)\n",
    "        #     ->\n",
    "        # after: (b, num_tokens, d_out)\n",
    "        #                        d_out = num_tokens * head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "\n",
    "        # combine the heads through a linear layer\n",
    "        # this is considered optional... why?\n",
    "        # TODO: appendix B for more details\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2fa6d88f-7b44-4990-b634-05032586d73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "triu(..., diagonal=0) retains the diag\n",
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "\n",
      "triu(..., diagonal=1) omits the diag\n",
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# reminding myself of behavior.. \n",
    "# ... diagonal=0 means diagonal values are RETAINED\n",
    "print(\"\\ntriu(..., diagonal=0) retains the diag\")\n",
    "print(torch.triu(torch.ones(context_length, context_length), diagonal=0))\n",
    "\n",
    "# ... diagonal=1 means diagonal values are OMITTED (0's)\n",
    "print(\"\\ntriu(..., diagonal=1) omits the diag\")\n",
    "print(torch.triu(torch.ones(context_length, context_length), diagonal=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7236372e-b5d8-430f-8db5-772e5824462f",
   "metadata": {},
   "source": [
    "Let's review the intuition behind batch matrix multiplication..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "669fbb93-9289-4826-8836-62692701168b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],   \n",
    "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "\n",
    "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "47a187c3-7e75-454f-b484-972819163dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.2745, 0.8993, 0.7179],\n",
      "          [0.6584, 0.0390, 0.7058],\n",
      "          [0.2775, 0.9268, 0.9156],\n",
      "          [0.8573, 0.7388, 0.4340]],\n",
      "\n",
      "         [[0.0772, 0.4066, 0.4606],\n",
      "          [0.3565, 0.2318, 0.5159],\n",
      "          [0.1479, 0.4545, 0.4220],\n",
      "          [0.5331, 0.9737, 0.5786]]]])\n"
     ]
    }
   ],
   "source": [
    "print(a.transpose(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "73917b2d-b7f4-498d-a89c-5a81c77bf98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "print(a @ a.transpose(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e841e5-fdcb-419e-9af4-7f2dd0e5ad57",
   "metadata": {},
   "source": [
    "This above single matrix multiplication is equal to computing each head independently. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "871027b8-b771-413c-a4d1-e81c8066e616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "Second head\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n"
     ]
    }
   ],
   "source": [
    "first_head = a[0,0,:,:]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    "\n",
    "second_head = a[0,1,:,:]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"Second head\\n\", second_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3358f0a0-e4d2-4ab7-a563-bc554d172367",
   "metadata": {},
   "source": [
    "Let's try running it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7610bce0-e3dd-411f-8fa1-8f351c72b6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a014affb-ebec-4599-a33d-4576db548855",
   "metadata": {},
   "source": [
    "_Exercise 3.3: Initializing GPT-2 size attention modules_\n",
    "\n",
    "Using the MultiHeadAttention class, initialize a multi-head attention module that has the same number of attention heads as the smallest GPT-2 model (12 attention heads). Also ensure that you use the respective input and output embedding sizes similar to GPT-2 (768 dimensions). Note that the smallest GPT-2 model supports a context length of 1,024 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "03c5987b-bada-438c-85a8-fc4ce36f4ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = 768\n",
    "d_out = 768\n",
    "context_length = 1024\n",
    "dropout_ratio = 0.0\n",
    "num_heads = 12\n",
    "\n",
    "gpt2 = MultiHeadAttention(d_in, d_out, context_length, dropout_ratio, num_heads=num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c2cf4e88-7f95-4282-a4b4-8df1b88365d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "gpt2_input = torch.rand((1024,768))\n",
    "print(gpt2_input.shape)\n",
    "print(gpt2_input[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "77270b75-df4d-4303-981a-1ef070316449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "gpt2_batch = torch.stack((gpt2_input, gpt2_input), dim=0)\n",
    "print(gpt2_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "fdc1650b-6de2-459f-a0bb-b71166433a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0685,  0.0206, -0.3180,  ...,  0.1948, -0.1485, -0.2868],\n",
       "         [-0.1240, -0.0154, -0.2823,  ...,  0.1384, -0.1513, -0.2925],\n",
       "         [-0.0590, -0.0165, -0.2872,  ...,  0.0375, -0.1005, -0.3074],\n",
       "         ...,\n",
       "         [ 0.0014, -0.0153, -0.2097,  ...,  0.0947, -0.0958, -0.3008],\n",
       "         [ 0.0013, -0.0153, -0.2097,  ...,  0.0941, -0.0960, -0.3007],\n",
       "         [ 0.0014, -0.0155, -0.2098,  ...,  0.0944, -0.0956, -0.3011]],\n",
       "\n",
       "        [[-0.0685,  0.0206, -0.3180,  ...,  0.1948, -0.1485, -0.2868],\n",
       "         [-0.1240, -0.0154, -0.2823,  ...,  0.1384, -0.1513, -0.2925],\n",
       "         [-0.0590, -0.0165, -0.2872,  ...,  0.0375, -0.1005, -0.3074],\n",
       "         ...,\n",
       "         [ 0.0014, -0.0153, -0.2097,  ...,  0.0947, -0.0958, -0.3008],\n",
       "         [ 0.0013, -0.0153, -0.2097,  ...,  0.0941, -0.0960, -0.3007],\n",
       "         [ 0.0014, -0.0155, -0.2098,  ...,  0.0944, -0.0956, -0.3011]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2(gpt2_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
